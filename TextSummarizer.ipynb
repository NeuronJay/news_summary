{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IITG_Summarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeuronJay/news_summary/blob/master/IITG_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab_type": "code",
        "id": "_Jpu8qLEFxcY",
        "outputId": "432419fe-1eb6-454d-ab90-fa1fff505685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KziSd-AypyvM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fef5ed8f-e411-41c1-9e02-6e66c72f0d94"
      },
      "source": [
        "url = 'https://github.com/NeuronJay/news_summary/blob/master/news_summary.csv'\n",
        "data = pd.read_csv(url, error_bad_lines=False)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Skipping line 78: expected 1 fields, saw 7\\nSkipping line 129: expected 1 fields, saw 3\\nSkipping line 146: expected 1 fields, saw 9\\nSkipping line 147: expected 1 fields, saw 3\\nSkipping line 171: expected 1 fields, saw 4\\nSkipping line 175: expected 1 fields, saw 2\\nSkipping line 177: expected 1 fields, saw 2\\nSkipping line 178: expected 1 fields, saw 2\\nSkipping line 179: expected 1 fields, saw 2\\nSkipping line 180: expected 1 fields, saw 2\\nSkipping line 181: expected 1 fields, saw 2\\nSkipping line 182: expected 1 fields, saw 2\\nSkipping line 183: expected 1 fields, saw 2\\nSkipping line 184: expected 1 fields, saw 2\\nSkipping line 185: expected 1 fields, saw 2\\nSkipping line 186: expected 1 fields, saw 2\\nSkipping line 190: expected 1 fields, saw 2\\nSkipping line 191: expected 1 fields, saw 2\\nSkipping line 197: expected 1 fields, saw 2\\nSkipping line 205: expected 1 fields, saw 4\\nSkipping line 211: expected 1 fields, saw 2\\nSkipping line 216: expected 1 fields, saw 2\\nSkipping line 217: expected 1 fields, saw 2\\nSkipping line 218: expected 1 fields, saw 2\\nSkipping line 219: expected 1 fields, saw 2\\nSkipping line 220: expected 1 fields, saw 2\\nSkipping line 225: expected 1 fields, saw 2\\nSkipping line 226: expected 1 fields, saw 2\\nSkipping line 227: expected 1 fields, saw 2\\nSkipping line 234: expected 1 fields, saw 2\\nSkipping line 242: expected 1 fields, saw 4\\nSkipping line 247: expected 1 fields, saw 2\\nSkipping line 250: expected 1 fields, saw 2\\nSkipping line 251: expected 1 fields, saw 2\\nSkipping line 255: expected 1 fields, saw 2\\nSkipping line 256: expected 1 fields, saw 2\\nSkipping line 278: expected 1 fields, saw 2\\nSkipping line 415: expected 1 fields, saw 9\\nSkipping line 416: expected 1 fields, saw 3\\nSkipping line 421: expected 1 fields, saw 9\\nSkipping line 422: expected 1 fields, saw 3\\nSkipping line 469: expected 1 fields, saw 9\\nSkipping line 480: expected 1 fields, saw 9\\nSkipping line 492: expected 1 fields, saw 9\\nSkipping line 625: expected 1 fields, saw 3\\nSkipping line 629: expected 1 fields, saw 3\\nSkipping line 630: expected 1 fields, saw 11\\nSkipping line 699: expected 1 fields, saw 2\\nSkipping line 765: expected 1 fields, saw 2\\nSkipping line 797: expected 1 fields, saw 2\\nSkipping line 798: expected 1 fields, saw 3\\nSkipping line 799: expected 1 fields, saw 3\\nSkipping line 800: expected 1 fields, saw 3\\nSkipping line 801: expected 1 fields, saw 3\\nSkipping line 802: expected 1 fields, saw 3\\nSkipping line 809: expected 1 fields, saw 3\\nSkipping line 810: expected 1 fields, saw 3\\nSkipping line 811: expected 1 fields, saw 3\\nSkipping line 812: expected 1 fields, saw 3\\nSkipping line 813: expected 1 fields, saw 3\\nSkipping line 814: expected 1 fields, saw 3\\n'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wnK5o4Z1Fxcj",
        "colab": {}
      },
      "source": [
        "#data = pd.read_csv('/content/drive/My Drive/Colab Notebook/Reviews.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "__fy-JxTFxc9",
        "outputId": "8072b21c-9d93-44be-ffe6-86a45f4dba5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 566 entries, 0 to 565\n",
            "Data columns (total 1 columns):\n",
            "<!DOCTYPE html>    566 non-null object\n",
            "dtypes: object(1)\n",
            "memory usage: 4.5+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wlaLbGZyrHb",
        "colab_type": "text"
      },
      "source": [
        "The below mentioned clean up process has been used from the semantic analysis document and has been explained there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id7vMRWRqeOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rem_stopwords(stop_words, tokens):\n",
        "    res = []\n",
        "    for token in tokens:\n",
        "        if not token in stop_words:\n",
        "            res.append(token)\n",
        "    return res\n",
        "\n",
        "contra = {\"y'all\": \"you all\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\",\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\"}\n",
        "\n",
        "def cleanup_text(text,num):\n",
        "    text = text.encode('ascii', errors='ignore').decode()\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', ' ', text)\n",
        "    text = re.sub(r'#+', ' ', text )\n",
        "    text = re.sub('\"','', text)\n",
        "\n",
        "    #contra used ( contraction mapping )\n",
        "    text = ' '.join([contra[t] if t in contra else t for t in text.split(\" \")]) \n",
        "\n",
        "    text = re.sub(r'@[A-Za-z0-9]+', ' ', text)\n",
        "    text = re.sub(r\"([A-Za-z]+)'s\", r\"\\1 is\", text)\n",
        "    #text = re.sub(r\"\\'s\", \" \", text)\n",
        "\n",
        "    text = re.sub('\\W', ' ', text)\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "        if(num==0):\n",
        "        tokens = [w for w in newString.split() if not w in stop_words]\n",
        "    else:\n",
        "        tokens=newString.split()\n",
        "    long_words=[]\n",
        "    for i in tokens:\n",
        "        if len(i)>1:                                                 #removing short word\n",
        "            long_words.append(i)   \n",
        "    return (\" \".join(long_words)).strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_AFGy5kx5st",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_text = []\n",
        "for t in data['text']:\n",
        "    cleaned_text.append(cleanup_text(t,0)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GsRXocxoFxd-",
        "colab": {}
      },
      "source": [
        "cleaned_summary = []\n",
        "for t in data['Summary']:\n",
        "    cleaned_summary.append(cleanup_text(t,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r7KHLMhXy7WQ"
      },
      "source": [
        "Checking at the first 10 preprocessed textual information and related summaries ! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jQJdZcAzFxee",
        "colab": {}
      },
      "source": [
        "cleaned_text[:10]\n",
        "cleaned_summary[:10]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L1zLpnqsFxey",
        "colab": {}
      },
      "source": [
        "data['cleaned_text']=cleaned_text\n",
        "data['cleaned_summary']=cleaned_summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MdF76AHHFxgw",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_word_count = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in data['cleaned_text']:\n",
        "      text_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
        "\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7JRjwdIOFxg3",
        "colab": {}
      },
      "source": [
        "cnt=0\n",
        "for i in data['cleaned_summary']:\n",
        "    if(len(i.split())<=50):\n",
        "        cnt=cnt+1\n",
        "print(cnt/len(data['cleaned_summary']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yYB4Ga9KMjEu"
      },
      "source": [
        "I observe that majority of the summaries to be 50 and the text to be around 300."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZKD5VOWqFxhC",
        "colab": {}
      },
      "source": [
        "max_text_len=300\n",
        "max_summary_len=50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E6d48E-8M4VO"
      },
      "source": [
        "Let us select the reviews and summaries whose length falls below or equal to **max_text_len** and **max_summary_len**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yY0tEJP0FxhI",
        "colab": {}
      },
      "source": [
        "cleaned_text =np.array(data['cleaned_text'])\n",
        "cleaned_summary=np.array(data['cleaned_summary'])\n",
        "\n",
        "short_text=[]\n",
        "short_summary=[]\n",
        "\n",
        "for i in range(len(cleaned_text)):\n",
        "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
        "        short_text.append(cleaned_text[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "        \n",
        "df=pd.DataFrame({'text':short_text,'summary':short_summary})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYag9OBqDmlG",
        "colab_type": "text"
      },
      "source": [
        "Unique name "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EwLUH78CFxhg",
        "colab": {}
      },
      "source": [
        "df['summary'] = df['summary'].apply(lambda x : 'janny '+ x + 'mohtarma')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oRHTgX6hFxhq",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "x_tokenizer = Tokenizer() \n",
        "x_tokenizer.fit_on_texts(list(x_tr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RzvLwYL_PDcx"
      },
      "source": [
        "Here, I am defining the threshold to be 4 which means word whose count is below 4 is considered as a rare word\n",
        "\n",
        "Rare words have to be focused on the frequency and orginating !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "So-J-5kzQIeO"
      },
      "source": [
        "tot_cnt gives the size of list of unique qords\n",
        " cnt gives no. of rare words whose count falls below threshold\n",
        "tot_cnt - cntgives top most common words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y8KronV2Fxhx",
        "colab": {}
      },
      "source": [
        "thresh=4\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "    \n",
        "print(\"RARE %:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of RARE\",(freq/tot_freq)*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J2giEsF3Fxh3",
        "colab": {}
      },
      "source": [
        "#currently working on preparing a tokenizer for reviews on training data"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
